---
title                 : "Developing a synthetic news corpus to validate generic frame detection methods"
shorttitle            : "FRAME DETECTION"
author:
  - name              : "Chung-hong Chan"
    affiliation   : "1"
    corresponding : yes
    address       : "Unter Sachsenhausen 6-8, 50667 Cologne, Germany"
    email : "chung-hong.chan@gesis.org"
  - name              : "Rainer Freudenthaler"
    affiliation   : "2"
  - name              : "Philipp Müller"
    affiliation   : "3"
    
affiliation:
  - id                : "1"
    institution   : "GESIS - Leibniz-Institut für Sozialwissenschaften"
  - id                : "2"
    institution   : "Mannheimer Zentrum für Europäische Sozialforschung"
  - id                : "3"
    institution   : "Institute for Media and Communication Studies, University of Mannheim"

authornote: |
  The link to the online appendix of this submission: https://osf.io/gkft5/. Preregistration:  https://doi.org/10.17605/OSF.IO/SY6JX. Code and data: https://github.com/chainsawriot/frame. The authors would like to thank 1) Valerie Hase (Ludwig-Maximilians-Universität München) for providing keywords to be used in this study, 2) Open Science Office, University of Mannheim, for the financial support, 3) Sandra Krais for language editing, and 4) student assistants: Filippo Borsato, Hannah Erb, Hyosun Jang, Fatih Ozhasar, Zeynep Özgülec, and Jonathan Vincent.

abstract: |
  Frames are a central concept in communication research. Based on our literature review, we propose that frame identification is an act of identifying selected reality and communicative intention. We then highlight the conceptual and methodological issues of frame identification using computational methods. To avoid the correlation between topics and frames, we provide a synthetic dataset for evaluating frames found in multi-topical news content, using the detection of generic frames as a test case. With this dataset, for the first time, we benchmark manual coding and various automatic and semi-supervised methods. Based on the preliminary benchmark results, this study provides evidence that generic frame identification using both manual coding and automatic methods might not be accurate.
 
keywords              : "frame, unsupervised method, topic model, semi-supervised method, validity"
wordcount             : "8774"
bibliography          : "frame.bib"
floatsintext          : yes
linenumbers           : no
draft                 : no
mask                  : no
figurelist            : no
tablelist             : no
footnotelist          : no
classoption           : "man"
output:
 papaja::apa6_pdf:
   latex_engine: xelatex
---

```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(brms)
library(here)
require(ggridges)

ireadRDS <- function(fname) {
        readRDS(here::here("intermediate", fname))
}
source("lib.R")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

The goal of this study is to synthesize an experimental dataset for evaluating generic frames found through different methods. In order to achieve this goal, we first review the concept of (generic) frames in communication research and then highlight the conceptual and methodological issues of (generic) frame identification. Then we outline our approach to synthesize such a dataset, analyze the dataset, and report the lessons learned. The most important lesson: Identification of (generic) frames from news content written by someone else is an incredibly difficult task, even for experts.

## Entmanian frame and the tacit aspect of communicative intention

The notion of (media) frame is probably one of the central concepts in communication research. As of writing, a simple keyword search of "Frame" returned 1278 results from *Journal of Communication* alone. In several journals of the field, special issues have been published to solely interrogate this central concept (e.g. *Journal of Communication* 57:1; *Media, War & Conflict* 11:4).

Before the onset of the so-called "Computational Turn" of journalism research [@hase:2022:CT] and the notion of automated content analysis [@boumans:2015:tst], methodological controversies surrounding the detection of frames have already been a greatly discussed topic even in the context of traditional manual content analysis. Even the concept itself has been defined and redefined by various experts. The contested [@dangelo:2002:NFM], but highly cited, definition by @entman:1993:F states that framing (an act, i.e. a verb) is "select[ing] some aspects of a perceived reality and mak[ing] them more salient in a communicating text, in such a way as *to promote* a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described." (p.52, emphasis added) By using "to promote" in active voice indicates that the act of framing in Entman's sense is conducted by the communicators, not the recipients.

We restate the Entmanian definition of *framing* (verb) to define the noun *frame*: A frame is the result of an act of selecting and making salient certain aspects of a perceived reality by a communicator whose intention is to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation. Our restatement makes explicit the tacit aspect of *communicative intention* in the original definition. Our restatement is also compatible with @scheufele:2006:FAS's "three models of political communication", which differentiate between framing, agenda setting, and priming. In their models, framing refers to "modes of presentation that journalists and other communicators use to present information in a way that resonates with existing underlying schemas among their audience." (p.12) The underlying communicative intention is then to "resonate with existing underlying schemes among their audience." Similarly, @baden2015infocore's notion of "interpretative frame" also focuses on strategic and constructive purposes. In psychology, the research on framing also deals with the communicative intention of influencing choices and decisions through different ways to represent the same information [e.g. @tversky:1981:FDP].

With this argument we do not mean to imply that communicators, journalists in particular, always think about which frame to choose and consciously adopt a specific frame for a story. Within framing theory, the framing process is understood as allowing for actors to unconsciously adapt frames that have been communicated by other actors. For example, the strategic framing of issues by politicians can lead to journalists adopting (consciously or unconsciously) a specific frame within their reporting [@matthes:2014:F, p.14-19]. A journalist, for instance, can pick up the economic frame on climate change and then frame their reporting in terms of the costs of mitigating climate change, without reflecting upon the fact that other ways to perceive the problem exist. Still, we argue, for a story to communicate a frame, it must communicate the intention contained within the frame, or the frame is not communicated. The journalist in this example would have to communicate the intention that climate change should be seen as an economic problem, even if they do not intentionally rule out other frames. Simply put: A frame contains communicative intention even if it does not express authorial intent.

Making the tacit aspect of communicative intention explicit raises several questions about frame detection. The most obvious is: What exactly is the act of detecting frames? Should it be judging which aspects of a perceived reality have been selected and emphasized by a communicator? Or judging the original communicative intention of the communicator from the text? We propose that frame detection is an act of detecting both (selected aspect of a perceived reality and communicative intention) and we can't tell a frame from texts by just detecting either one. We explain this problem by a visual metaphor (Figure \@ref(fig:fig1)).

```{r fig1, echo = FALSE, fig.cap = 'A metaphor for frame detection'}
knitr::include_graphics(here::here("figure", "fig1.png"))
```

Suppose the crowd giving the Nazi salute is the reality. If we detect the man who does not give the Nazi salute (a selected aspect of a perceived reality) and claim that to be a "frame" (Case A), it might resonate with the audience but with an ambiguous communicative intention: a *Gestapo* officer might select this aspect with the communicative intention to ask other citizens to hunt for this man; or a resistance fighter might select this aspect with a communicative intention to save this man or to ask other citizens to be like this man.

In another direction, we might know the communicative intention of the communicator. However, this communicator selects an arbitrary aspect to convey their intention (Case B). This cannot be a "frame" either because the selected aspect does not convey their communicative intention. Only after both the selected aspect of a perceived reality and the communicative intention are detected can we unambiguously say what the frame is (Case C).

## The many approaches of identifying frames empirically/computationally

There has been criticism on how researchers detect frames empirically. @carragee:2004:NPR criticize that some researchers "reduce frames to story topics, attributes, or issue position." (p.217) A more harsh criticism from @reese:2007:FP is for framing researchers to "give an obligatory nod to the literature before proceeding to do whatever they were going to do in the first place." (p.151) Against this background, it comes as no surprise that the systematic review by @matthes:2009:WF identifies a great variety of operationalization and reporting standards in empirical framing research, despite the fact that most of the considered studies are referencing the same conceptual framing literature.

In the realm of content analysis, @matthes:2008:CAM suggest there are five different approaches for the identification of frames: hermeneutic approach, linguistic approach, manual holistic approach, computer-assisted approach, and deductive approach. It is important to note that the approach outlined in @matthes:2008:CAM is computer-*assisted* approach and the exemplar models are dictionary-based approaches such as @miller:1997:FMA.

A relatively new approach is to apply unsupervised machine learning techniques to find frames through induction. As of writing, we are able to find several methods papers suggesting that these unsupervised machine learning techniques can be used to find frames [@dimaggio:2013:E;@nicholls:2020:CIM;@burscher:2016:FBW;@greussing:2017:S;@walter:2019:NFA;@eisele:2023:CNF]. Not surprisingly, all, except @dimaggio:2013:E, have given the "obligatory nod" ---in @reese:2007:FP's sense--- to @entman:1993:F.

Before diving into the methodological rationale behind frame detection with unsupervised machine learning techniques, we revisit what unsupervised machine learning does. Unsupervised machine learning can be divided into three categories: dimensionality reduction, clustering, and density estimation. The first two are focused here because the aforementioned frame detection techniques do not utilize the last approach. Text data represented in the traditional bag-of-word method has a high dimensionality in the feature space. Dimensionality reduction, which the method proposed by @greussing:2017:S is based on, attempts to reduce the dimensionality in the feature space, yet retains a maximum amount of information from the original data. Clustering analysis, which the methods evaluated or proposed by @dimaggio:2013:E, @nicholls:2020:CIM, @burscher:2016:FBW, and @walter:2019:NFA are based on, attempts to find groups within high-dimensional data, yet members of the same group have a minimum of variance between each other. These differences notwithstanding, all unsupervised methods attempt to find potentially meaningful clusters of words through either maximizing the information or reducing the internal variance among members of a cluster. All methods do not involve any labeled data, thus these methods are fully automatic and inductive. These methods are referred to as "automatic inductive methods" in the rest of this article. Often, however not always, the clustering solution from these automatic inductive techniques are *posthoc* validated against human-coded data to show that the resulting solution is indeed meaningful. Also, it is noteworthy that these inductive methods originally were not developed to capture frames, but mostly to identify topics within documents. Therefore, topic modeling is an alternative name for the inductive methods which @dimaggio:2013:E, @nicholls:2020:CIM, and @walter:2019:NFA are based on. Here, already the question is: How are the patterns that we detect here ---clusters of words that commonly occur together--- related to intent in the sense we ascribe to the framing process?

The papers referenced above have given different answers. @dimaggio:2013:E suggest that "many topics may be viewed as frames (semantic contexts that prime particular associations or interpretations of a phenomenon in a reader)" (p.578), which might be conflating the two concepts. Other papers argue that the word clusters that result from unsupervised clustering have semantic meanings. @greussing:2017:S suggest that word clusters "are networks of co-occurring words, constituting the semantic patterns in which words are used, and capturing the underlying structures that provide meaning to a text." (p.1755)  @walter:2019:NFA find "several reasons to believe topics could *not* be conceptualized as news frames." (p.5, emphasis added) They propose a solution to "connect topics into larger themes or meta-topics." (p.5) In order to do that, they propose their Analysis of Topical Model Networks (ANTMN) approach, a three-step process of topic modeling, network analysis, and community detection. On the question of whether or not the "larger themes or meta-topics" found through their approach can be interpreted as frames, they argue that "the methodological steps ANTMN follows are consistent with the conceptualization of frames as news patterns." (p.14) But the definition of framing they cite from Entman is news patterns that "repeatedly invoke the same objects and traits, using identical or synonymous words and symbols in a series of similar communications that are concentrated in time", a broader definition that would include patterns that do not promote a specific problem definition and are agnostic towards authorial intent [@walter:2019:NFA, p.261]. 

There are also authors who do not agree with the interpretation of word clusters as frames. @jacobi:2018:Q, in their guide on LDA in journalism research, caution that word clusters (topics) are not "interpretive packages." Similarly, @guo:2022:POS maintain that word clusters are not equivalent to frames. Specifically, their criticism to @walter:2019:NFA's approach is that the so-called "frames" identified with the authors' method do not match the required criteria of what constitutes a frame according to the existing media framing literature. @guo:2022:POS make a distinction between "topic-like frames", e.g. "Safety of nuclear plants" in @burscher:2016:FBW, and media frames from a constructive perspective, e.g. generic frames in @semetko:2000:FE. @guo:2022:POS argue that inductive methods are only capable of detecting "topic-like frames." @hase:2022:CT use how the output of these methods being interpreted as an example of "trivialization of theories/concepts [^banalisierung]" in computational communication science.

[^banalisierung]: Originally in German: *Banalisierung von Theorien/Konzepten.*

The (in)sufficiency of semantic meanings as evidence of frame detection can also be illustrated using the visual metaphor in Figure \@ref(fig:fig1). We can say that case A has a clear semantic meaning (there is a man who does not give the Nazi salute). But the communicative intention remains unknown. Applying @scheufele:2006:FAS's "three models of political communication", those "topic-like frames" should rather be subsumed under issues or agendas, not frames. Or more appropriately: subtopics of a news topic. The question we therefore raise is: Can the categories we inductively generate from a text corpus capture the authorial intent to promote a specific problem definition, or do they capture something else?

## Generic frame detection as a test case and ensuring independence from topics

For this paper we decided to use generic frames as a test case to measure how well different methods can reproduce authorial intent from a given corpus. Generic frames, as defined by @de2005news, are frames that "transcend thematic limitations and can be identified in relation to different topics, some even over time and in different cultural contexts." (p.54) This has two advantages: First, we can rely on an available definition of the frames to instruct authors to write articles for us that contain a specific generic frame. We therefore have access to the ground truth of authorial intent.

Meanwhile, detection of generic frames is challenging for automated/automatic methods: @nicholls:2020:CIM evaluate different automatic inductive methods and show that STM [@roberts:2014:STM] is capable of detecting frames in a corpus with a narrow scope. But the method extracts topics rather than frames in another corpus with a broad scope. Using the typology by @de2005news, this reflects the distinction between issue-specific frames and generic frames. The finding by @nicholls:2020:CIM effectively bars STM or relative inductive methods from detecting generic frames. As a matter of fact, all examples used in the original methods papers were limited to one news topic [e.g. art in @dimaggio:2013:E ; refugee coverage in @greussing:2017:S ; and financial news in @nicholls:2020:CIM]. On the other hand, inductive methods have been applied to multi-topical news corpora before with the goal to identify generic frames [e.g. @walter:2022:WWW].

A second advantage is that generic frames, in theory, can be combined with any topic, and we can combine them without introducing correlation. In real-world settings, frames (both issue-specific frames and generic frames) often correlate with topics. For example, @iyengar1994anyone observes for episodic versus thematic framing that the episodic frame is applied more frequently in crime stories but not in terrorism (by foreign actors and left-wing perpetrators) stories. The opposite is true when it comes to the thematic frame. 

There is no doubt that automatic inductive methods can distinguish crime stories from terrorism stories. However, it is also possible to shoehorn these two topics found in a multi-topical corpus into a reasonably accurate indicator of episodic and thematic frames. Frame detection would therefore be incidental to what the method actually measures. It is like measuring the consumption of chocolate within a country as an indicator of scientific advancement. The indicator might be associational (since chocolate consumption rises with economic prosperity, and economic prosperity correlates with scientific advancement), but not causal. This problem also manifests itself in a single-topic situation, e.g. terrorism, which is usually discussed in media discourse using thematic framing. A single news topic usually has subtopics such as Islamist terrorism, left-wing terrorism, and right-wing terrorism. However, the shoehorned indicator breaks when the *episodic* frame is applied in right-wing terrorism stories ("lone wolf") in Western media [@zdjelar:2021:L;@hase:2021:W].

Therefore, for a method to sufficiently detect frames ---generic or not---, this method should be able to detect frames independent of topics. For example, if a method is proposed to detect episodic and thematic frames, this method should be able to really tell the differences between episodic and thematic frames in both right-wing and Islamist terrorism stories. But it should not be picking up the right-wing terrorism stories and then claim them to be episodic.

# Our approach: a synthetic dataset

After we have given the "obligatory nod" [@reese:2007:FP] to framing literature, we propose our approach. In order to test whether a method can reliably detect generic frames that correspond to authorial intent, we need to have a dataset in which the frames and topics are completely independent. This distribution is not natural, but can be generated by randomization. Also, there is no guarantee that manual coding, the so-called "gold standard" of frame detection [@nicholls:2020:CIM], can actually "reverse-engineer" communicative intentions as perceived by an audience member. Therefore, we need to synthesize a dataset where frames (the package of selected aspects of perceived reality and communicative intention) are independent of topics all the way back to the communicative intention. This allows us to test whether our frame induction methods are able to find the intended frames without ---for human coders--- relying on intuitions about which frames commonly occur within certain topics or ---for automatic methods--- picking up frames incidentally as a by-product of actually generating distinct topics. Such a synthetic approach has previously been used by @clever:2020:ADN and @frischlich2022populists to solve a similar problem (evaluation of nostalgia detection).

```{r chi, include = FALSE}
rio::import(here::here("data", "Frame Corpus.xlsx")) %>% tibble::as_tibble() -> frame_df
chisq_test <- suppressWarnings(chisq.test(frame_df$frame, frame_df$topic))
```

We randomly assigned `r nrow(frame_df)` pairs of topics and frames (Table \@ref(tab:tab1)). The topics were "Ukraine," "corona," "tech companies," "climate," and "any topic." For the frames we used the generic frames following @semetko:2000:FE : "Attribution of responsibility," "Human interest," "Conflict," "Morality," and "(Economic) consequences." The topics and frames are independent (${\chi}^2$ = `r round(chisq_test$statistic[['X-squared']], 3)`, df = `r chisq_test$parameter[['df']]`, p = `r round(chisq_test$p.value, 3)`). In the subsequent sections, these are called "ground truth topics" ($z$) and "ground truth frames" ($y$) respectively.

```{r tab1}
rio::import(here::here("data", "Frame Corpus.xlsx")) %>% tibble::as_tibble() -> frame_df
frame_df$frame[frame_df$frame == "Economic Consequences"] <- "Conseq."
frame_df$frame[frame_df$frame == "Human Interest"] <- "Hum. Int."
frame_df$frame[frame_df$frame == "Responsibility"] <- "Resp."
frame_df %>% group_by(topic, frame) %>% summarise(n = n()) %>% spread(frame, n) %>% ungroup %>%
        papaja::apa_table(caption = "Distribution of topics and frames")
```

We gave these ground truth frame-topic pairs to four authors [political science master students with prior knowledge concerning framing theory and generic frames as proposed by @semetko:2000:FE] as stimuli and instructed them to write news articles containing the assigned topics and frames. These authors were also randomly paired up to edit the articles written by their peers to ensure the articles were actually conveying the assigned topics or frames. The instruction (in verbatim) given to the four authors regarding the criterion for an article containing a frame when editing articles is as follows: "You would check at least one item of a specific frame [see @semetko:2000:FE]." In other words, the four authors wrote and edited the articles with a very specific communicative intention of framing the topics in a specific generic frame such that at least one item of the codebook [@semetko:2000:FE] would be checked.

Through this process we generated a multi-topical corpus of `r nrow(frame_df)` news articles with orthogonal ground truth frames and topics. The ground truth of frames contained within these `r nrow(frame_df)` multi-topical news articles are known even without manual coding.

# An application: A preregistered preliminary analysis of generic frame identification methods 

As a use case of the synthetic dataset, we selected different methods and attempted to identify frames in those `r nrow(frame_df)` multi-topical news articles. As there are many methodological limitations in this benchmark, please consider this benchmark as preliminary.

## Hypotheses

We tested three preregistered hypotheses:

*H1: Compared with manual methods, automatic inductive methods are less accurate in detecting frames.*

*H2: Compared with semi-supervised methods, automatic inductive methods are less accurate in detecting frames.*

*H3: Compared with manual methods, semi-supervised methods are less accurate in detecting frames.*

## The "gold standard"

Two coders (two other political science Master students) were instructed to manually code the `r nrow(frame_df)` articles to find the frame elements of each news item using the codebook by @semetko:2000:FE (The complete codebook is available in the Online Appendix). One item (*"Does the story contain visual information that might generate feelings of outrage, empathy-caring, sympathy, or compassion?"*) was omitted because no images are generated in our synthetic approach. These two coders underwent two rounds of pretesting and training, prior to the actual coding. Despite pretesting and training, the intercoder reliability between these two coders was still low for some items, based on the test coding of 10 articles (See Online Appendix).

### Exploratory analysis: Expert coding

This part of the analysis has not been pre-registered and was planned after the above "gold standard" coding. After observing surprisingly low correct classification rates for student coders, as an exploratory analysis we studied whether the "gold standard" can be improved by using expert coding instead of the traditional two trained coders [@atteveldt:2021:VSA]. Two experts with PhD in communication were invited to repeat the above manual coding task. In addition, two items were added. The first item "F1" asks the frame of article in an exclusionary manner: *"Overall: The frame of this story is"* with five possible generic frames. This item is called "exclusionary item" because this item assumes a story can only have one frame. The second item "F2" asks the confidence of the answer for "F1": *My level of confidence for F1 is:* with a five-point Likert scale from Very Low to Very High.

As this part of the analysis has not been preregistered, the results were not used to test our preregistered hypotheses. Instead, the expert coding was used to study how experience and knowledge can influence the "gold standard"; also the item "F2" was used to study how the confidence level of the coders can possibly influence the correctness.

A further exploratory study of "ground truth contestion" was conducted to study whether the two experts agree with the ground truth, given their codings. That is, they were asked to assess whether an article matched the generic frame it was intended to convey after this frame was revealed to them.

## Automatic inductive methods

All automatic methods that have been claimed of being able to induce frames were investigated. This includes k-Means with TF-IDF [@burscher:2016:FBW], Principal Component Analysis with TF-IDF [@greussing:2017:S], LDA [@dimaggio:2013:E; evaluated by @eisele:2023:CNF], STM [@nicholls:2020:CIM], and Topic Model Networks [@walter:2019:NFA]. The number of clusters to find (k) was five. See Online Appendix for an overview of all included methods.

## Semi-supervised methods

We also investigated semi-supervised methods. This consists of Seeded-LDA [@watanabe:2020:TDA] and Keyword Assisted Topic Model [keyATM, @eshima2020keyatm]. It is important to clarify that the authors of these methods do not claim that their semi-supervised methods can be used for detecting frames. But both methods are claimed to be able to measure theoretical constructs through the provision of theory-driven dictionaries. @eisele:2023:CNF applied KeyATM to detect frames but found it unfit for the task. Therefore, semi-supervised methods were included in this study for exploratory purposes only. To apply these methods, we needed dictionaries that should be able to find the five generic frames. Before data collection, we surveyed two experts of journalism studies and pre-registered the dictionaries they suggested [^induce].

[^induce]: With the authors of the articles and the human coders working deductively - based on predefined categories derived from theory - and computational methods working inductively, human coders should already be at an advantage, of course: They know what characteristics of the text to look for and know what Frames to distinguish. Meanwhile, inductive methods generate the categories based on common patterns within the text corpus - so they have the disadvantage that they could find other, albeit also meaningful, patterns in the text. This of course opens up additional methodological questions (e.g.: What would be the ground truth for a category that is derived inductively from the text?). For this paper, we confine ourselves to the more limited question to answer "Do our methods find the same Frames that the authors consciously put into the text?" and will bracket the more sophisticated question "If inductive methods find something else, is that something else also meaningful?"

## Evaluation: Multiverse analysis

For automatic inductive methods, many methodological decisions need to be made: There are different ways to preprocess the text data [@maier:2018:ALT]. Even for the "gold standard," there are many methods to combine the frame elements into frames, despite the standard codebook [e.g. averaging by @dirikx:2010:T; Factor analysis by @haenens:2001:FD; binary categorization by @kroon:2022:BCW]. We preregistered all possible combinations of analytical steps and benchmark these methods with all possible combinations of methodological decisions using multiverse analysis [@steegen:2016:ITT;@pipal:2022:IYH]. For example, STM was applied using all combinations of possible preprocessing steps: 1) stemming vs lemmatization vs no processing, 2) removal of stopwords or not, 3) removal of sparse and dense words or not, 4) different levels of $\alpha$.

The original developers of Topic Model Networks have provided a careful justification of text preprocessing and model parameters [e.g. @walter:2019:NFA]. For it, we highlight the prescribed preprocessing suggestions in the multiverse analysis.

### "Best-case" Correct Classification Rate

To assess the accuracy of each method, we used the correct classification rate ($CCR$). Let $y$ be the ground truth frame vector $y$ and $\hat{y}$ to be the output from a method. $\hat{y}$ is a vector of frame indicators, $f_{k}$ where $k = 1,2,...,5$. $CCR$ is defined as $CCR = Pr(y = \hat{y})$.  However, there is no way to tell which frame indicator $f_{k}$ corresponds to which actual frame in the ground truth $y$. The usual practice is for a human rater to evaluate the topic words or visualization such as LDAvis [@sievert:2014:L] and map $f_{k}$ to the specific frame in $y$ accordingly [@maier:2018:ALT]. Several of these automatic inductive methods suggest human intervention at this stage. @walter:2019:NFA's method, marketed as an "inductive mixed-method," also has this mapping of detected clusters from multi-topical news content to the generic frames [@walter:2022:WWW]. There have been concerns about the validity of this approach [e.g. @chan:oolong] and we do not want the variation in these manual mapping decisions to influence our benchmark results.

Inspired by the calculation of best case complexity in the analysis of algorithms, we calculated what we called the "best case" CCR ($CCR_{max}$) using exhaustive search. In this analysis, we generate all possible permutations of all possible values of $k$, i.e. $\epsilon f_1, f_2, ..., f_k$. For $k = 5$, there are 120 possible permutations. For each of these 120 possible permutations, we calculated the $CCR$. From these 120 possible values, we selected the highest value, i.e. $CCR_{max}$, to represent the best-case scenario. This analysis is "unrealistic" in the sense that the ground truth is never known in real life. But this "best-case" analysis ensures that the real-life performance of these methods is equal to or in most cases worse than the $CCR_{max}$ reported, but never better. Therefore, the findings from this paper cannot be defended by the lack of human interpretation or any intervention. We have assumed that there were a *divinus* who can always perform the best in this mapping task.

The null value for $CCR_{max}$ is 0.2, when $k = 5$ [@krippendorff:2011:AIR]. It is also possible to calculate the same null $CCR_{max}$ value when a method can perfectly tell topics and then shoehorn those topics into frames. This expected $CCR_{max}$ value should also be 0.2 theoretically, when frames and topics are randomly assigned and the sample size is large. But due to the small sample size and idiosyncrasy of randomness, the *de facto* null value of $CCR_{max}$ is 0.3 in our `r nrow(frame_df)` frame-topic pairs. This value was found using the same exhaustive search technique by using the ground truth topics $z$ as $\hat{y}$ to map into the ground truth $y$ [^calculationtable]. In the figures below, we indicate both null values. One can think about the two null values as "can tell neither topics nor frames from the data" and "can tell topics but not frames from the data" respectively.

[^calculationtable]: One can also look at Table \@ref(tab:tab1) and use the so-called Greedy Algorithm to derive the *de facto* $CCR_{max}$. We first go after the largest numbers in the table: there are two cells with seven and use that as the initial mapping (mapping "Climate" to "Human Interest;" and "Tech Companies" to "(Economic) Consequences"); we left with three topics: ("Joker", "Ukraine", and "Corona") and three frames ("Conflict", "Morality", "Responsibility"). We repeat the mapping of the largest numbers until all frames and topics are mapped. In the end, we have a complete mapping and the numbers are 7,7,6,5,5. Their sum is `r 7+7+6+5+5` and dividing it by the total number of articles `r nrow(frame_df)` gives `r (7+7+6+5+5) / nrow(frame_df)`.

# Results

## "Gold Standard"

Figure \@ref(fig:fig2) shows the result of multiverse analysis of the "Gold Standard." In the ridgeline figure [@ggridges], the entire distribution of $CCR_{max}$ (based on binomial distribution) is shown. The multiverse analysis suggests that the "Gold Standard" can detect frames in the multi-topical news content better than null in almost all situations, regardless of methodological decisions, as indicated by the extremely little overlapping between the distributions of $CCR_{max}$ and the two null values. However, the performance is not as high as one would expect. The best of the best value of $CCR_{max}$ is only around .5.

```{r fig2, fig.cap = "Multiverse analysis of the \"Gold Standard\" (Orange: Not pre-registered, coding from expert(s); dotted lines: nulls)"}
set.seed(12121)
bind_rows(ireadRDS("expert_accuracy.RDS"), ireadRDS("human_accuracy.RDS")) %>% mutate(expert = c(rep(TRUE, 7), rep(FALSE, 5))) %>% arrange(maxp) %>% mutate(desc = reorder(desc, maxp)) %>% uncount(15000) %>% mutate(value = rbinom(n(), 100, maxp) / 100) %>% ggplot(aes(x = value, y = desc, fill = expert)) + geom_density_ridges(alpha = 0.5, linetype = "blank") + xlim(0, 1) + geom_vline(aes(xintercept = 0.2, alpha = 0.5), linetype = "dashed") + geom_vline(aes(xintercept = 0.3, alpha = 0.5), linetype = "dashed") + xlab(expression(CCR[max])) + ylab("Treatment") + scale_fill_brewer(palette="Dark2") +  theme_minimal() + theme(legend.position = "none") -> fig_human
ggplot2::ggsave(here::here("figure", "fig2.png"), fig_human)
knitr::include_graphics(here::here("figure", "fig2.png"))
```

This analysis also reveals that there is not enough evidence to show that using experts instead of trained coders increases the accuracy. The same can be said about coding frame elements and coding frame as a single item. In the Online Appendix<!-- ([https://osf.io/gkft5/?view_only=8bf2a34000a64c5fa5d645c3f37f681a](https://osf.io/gkft5/?view_only=8bf2a34000a64c5fa5d645c3f37f681a)) -->, a comparison of confidence level of the expert coders between correct and incorrect answers is presented. Experts could give incorrect answers confidently.

## Analysis of incorrect answers from two experts

```{r overall, echo = FALSE, message = FALSE}
#require(quanteda)
#require(spacyr)
#require(tidyverse)
set.seed(1212121)
rio::import(here::here("data", "Frame Corpus.xlsx")) %>% tibble::as_tibble() -> frame_df

ipath <- function(fname) {
        here::here("intermediate", fname)
}

fpath <- function(fname) {
        here::here("figure", fname)
}

frame_corpus <- corpus(x = frame_df$Content, docnames = frame_df$docid, docvars = data.frame(frame = frame_df$frame))

expert1 <- rio::import(here::here("data", "coding PM.xlsx")) %>% select(-2, -1)
expert2 <- rio::import(here::here("data", "coding RF.xlsx")) %>% select(-docid, -Content)

colnames(expert1) <- str_extract(colnames(expert1), "^[A-Z][0-9]")
colnames(expert2) <- str_extract(colnames(expert2), "^[A-Z][0-9]")

expert1_frame <- c("Responsibility", "Human Interest", "Conflict", "Morality", "Economic Consequences")[1 + expert1$F1]
expert2_frame <- c("Responsibility", "Human Interest", "Conflict", "Morality", "Economic Consequences")[1 + expert2$F1]

#table(human = expert1_frame, gt = frame_df$frame)

table(c(frame_df$frame, frame_df$frame), c(expert1_frame == frame_df$frame, expert2_frame == frame_df$frame)) %>% as.data.frame() %>% group_by(Var1) %>% mutate(t = sum(Freq), p = Freq / t) %>% ungroup %>% filter(Var2 == "TRUE") %>% select(Var1, p) %>% arrange(p) %>% rename("Frame (Ground Truth)" = `Var1`, "Overall" = `p`) -> overallp

table(frame_df$frame, expert1_frame == frame_df$frame)%>% as.data.frame() %>% group_by(Var1) %>% mutate(t = sum(Freq), p = Freq / t) %>% ungroup %>% filter(Var2 == "TRUE") %>% select(Var1, p) %>% arrange(p) %>% rename("Frame (Ground Truth)" = `Var1`, "Expert A" = `p`) -> expert1p

table(frame_df$frame, expert2_frame == frame_df$frame)%>% as.data.frame() %>% group_by(Var1) %>% mutate(t = sum(Freq), p = Freq / t) %>% ungroup %>% filter(Var2 == "TRUE") %>% select(Var1, p) %>% arrange(p) %>% rename("Frame (Ground Truth)" = `Var1`, "Expert B" = `p`) -> expert2p

suppressMessages(overallp %>% left_join(expert1p) %>% left_join(expert2p)) -> overalltable

papaja::apa_table(overalltable, caption = "Correct Classification Rates of two experts by ground truth frames")
```

Using the exclusionary item and the ground truth frames, we conducted a non-preregistered exploratory analysis of inaccurate answers from the two experts. Table \@ref(tab:overall) shows the $CCR$ values across all ground truth frame categories. Both experts were relatively good at identifying the "Economic Consequences" frame. However, the two experts were relatively worse at identifying the Morality frame.

In a *posthoc* manner, we also attempted to explain the disagreement between the ground truth and the expert judgment by asking the experts to comment on whether or not the ground truth is justified in the synthetic corpus. The two experts were given $y$ and their $\hat{y}$ and were asked whether they agree with the ground truth using a 5-point Likert score (1 = Strongly disagree, 5 = Strongly agree). They were also asked to provide comments about the articles in an open-ended question.

Figure \@ref(fig:fig3) displays distributions of scores from the two experts. In general, the two experts agree with the ground truth of the Economic Consequences, Conflict, and Human Interest frames, but not the Morality and Responsibility frame. The modal values from the two experts for Morality articles tend towards disagreement in general. Expert B’s modal value for Responsibility articles also indicates disagreement.

In open-ended responses, the two experts expressed two main concerns: (1) For articles with the ground truth Morality frame, they found the moral message ---"religious tenets or moral prescription" in the original definition by @semetko:2000:FE --- is unclear; and (2) They found elements of multiple frames in one article.

```{r fig3, fig.cap = "Distribution of ground truth agreement scores from two experts", echo = FALSE}
require(tidyverse)
require(ggridges)

rio::import(here::here("data", "frame_contestation_coded.xlsx")) %>% tibble::as_tibble() -> contest
colnames(contest) <- c("docid", "topic", "writer", "editor", "Content", "frame", "expert1", "expert2", "expert1a", "expert2a", "expert1b", "expert2b")

##contest %>% mutate(a_avg = (expert1a + expert2a) / 2) %>% ggplot(aes(x = a_avg, y = frame)) + geom_density_ridges(alpha = 0.5, panel_scaling = TRUE, linetype = "blank")

contest %>% select(frame, expert1a, expert2a) %>% pivot_longer(!frame, names_to = "expert", values_to = "score")  %>% mutate(expert = recode(expert, expert1a = "Expert A", expert2a = "Expert B")) %>% group_by(frame, expert, score) %>% count() %>% group_by(frame, expert) %>% mutate(t = sum(n), p = n / t) %>% ungroup %>% mutate(frame = recode(frame, "Economic Consequences" = "Econ. Conseq.")) %>% ggplot(aes(x = score, y = p)) + geom_bar(stat = "identity") + facet_grid(expert~frame) + xlab("Score (Do you agree with the ground truth frame of this article?)") + ylab("Proportion of articles") + theme_minimal() -> fig_contest
ggplot2::ggsave(here::here("figure", "fig3.png"), fig_contest, width = 7.2)
knitr::include_graphics(here::here("figure", "fig3.png"))
```

```{r fig4, fig.cap = "Distribution of best-case correct classification rates by methods (dotted lines: nulls)", echo = FALSE}
require(tidyverse)
require(ggridges)
ireadRDS <- function(fname) {
        readRDS(here::here("intermediate", fname))
}

bind_rows(mutate(ireadRDS("KM.RDS"), method = "K-Means"),
              mutate(ireadRDS("PCA.RDS"), method = "PCA"),
              mutate(ireadRDS("LDA.RDS"), method = "LDA"),
              mutate(ireadRDS("STM.RDS"), method = "STM"),
              mutate(ireadRDS("ANTMN.RDS"), method = "ANTMN"),
              mutate(ireadRDS("SEEDED.RDS"), method = "Seeded LDA"),
              mutate(ireadRDS("KEYATM.RDS"), method = "keyATM")) %>%
        mutate(maxp = map_dbl(res, max)) %>% mutate(method = factor(method)) %>% mutate(method = fct_relevel(method, "K-Means")) -> all_uni

human <- ireadRDS("human_accuracy.RDS") %>% mutate(method = "Gold")
all_uni %>% bind_rows(human) %>% mutate(method_type = case_when(method == "Gold" ~ 0,
                               method %in% c("keyATM", "Seeded LDA") ~ 1,
                               TRUE ~ 2)) %>% mutate(method_type = factor(method_type, levels = c(0,1,2), labels = c("Gold", "Semisupervised", "Automatic"))) -> all_uni

all_uni$recommend <- FALSE
all_uni[all_uni$method == "ANTMN" & all_uni$words == "none" & all_uni$trim & all_uni$stopwords,]$recommend <- TRUE

all_uni %>% mutate(method = fct_relevel(method, "Gold", "Seeded LDA", "keyATM")) %>% ggplot(aes(x = maxp, y = method, fill = method_type)) + geom_density_ridges(alpha = 0.5, panel_scaling = TRUE, linetype = "blank") + geom_density_ridges(aes(point_color = recommend), jittered_points = TRUE, position = position_points_jitter(width = 0.05, height = 0), point_shape = '|', point_size = 3, point_alpha = 1, alpha = 0, panel_scaling = TRUE, linetype = "blank") + geom_vline(aes(xintercept = 0.3, alpha = 0.5), linetype = "dashed") + geom_vline(aes(xintercept = 0.2, alpha = 0.5), linetype = "dashed") + xlim(c(0, 0.7)) + xlab(expression(CCR[max])) + ylab("Method") + theme_minimal() + scale_discrete_manual("point_color", values = c("#B0B0B0", "#FF0000"), guide = "none") + theme(legend.position = "none") -> fig_dist
ggplot2::ggsave(here::here("figure", "fig4.png"), fig_dist)
knitr::include_graphics(here::here("figure", "fig4.png"))
```

## Preliminary comparison of methods

Individual results from the multiverse analysis for each method are available in the Online Appendix. Figure \@ref(fig:fig4) displays the overall results of the multiverse analysis. The results from different methodological decisions are combined as density masses.

K-means, PCA, STM have no to extremely little overlap in density mass with human coding. ANTMN, keyATM, LDA, and Seeded LDA (in that order) have a relatively higher overlap in density mass with human coding.

By considering the entire distribution, H1 (automatic methods perform worse than human coding) appears to be supported, although LDA and ANTMN might be comparatively better with some methodological decisions. There is not enough evidence to support H2 (automatic methods perform worse than semi-supervised methods) and H3 (semi-supervised methods perform worse than human coding).

# Discussion

Based on our review of the framing literature, we provide a synthetic benchmark dataset for detection of generic frames where frames and topics are independent. As an application of this synthetic dataset, we evaluated the "gold standard"[@semetko:2000:FE], fully-automated clustering methods that are claimed to be able to detect frames inductively [@burscher:2016:FBW;@greussing:2017:S;@nicholls:2020:CIM;@walter:2019:NFA], and two semi-supervised methods [@watanabe:2020:TDA;@eshima2020keyatm].

Using the multiverse analytic approach [@steegen:2016:ITT;@pipal:2022:IYH], we exhaustively studied all methods, irrespective of methodological choices, and reported their "best-case" performance. 

In the following paragraphs, we will summarize the surprising findings of our analyses, particularly regarding the human "gold standard" coding. Then we discuss possible reasons for our results, first on the level of article generation, second concerning measurement, then we discuss potential conceptual issues. Lastly, we note limitations of our current approach and give suggestions for further research.

For the first time, the current study is able to benchmark the so-called "gold standard": manual content analysis for frame identification. Although the "gold standard" performs significantly better than null, the performance is not superb (Figure \@ref(fig:fig2)). There have been concerns about the reliability of @semetko:2000:FE's codebook [e.g. @kroon:2022:BCW]. But the less-than-superb performance cannot be explained by intercoder variations alone because the multiverse analysis has considered both the single- and double-coder scenarios. This low performance is not what we expected and also thought-provoking: Given the fact that the "gold standard" can only detect 50\% of frames correctly and the state-of-the-art supervised classifiers classify frames at around 60\% accuracy, should we trust the supervised frame classifiers trained on the so-called "gold standard" data [e.g. @kroon:2022:BCW;@liu2019detecting;@kwak2020systematic;@eisele:2023:CNF]? We do not have an empirical answer to this question, because we did not study supervised methods in this paper.

Our findings do point to one important fact: Identification of (generic) frames from news content written by someone else is an incredibly difficult task, even for experts (Figure \@ref(fig:fig2)). This task is so complex because we need to evaluate the semantics of the selected reality as well as the communicative intention of a third party. Prediction of a communicator's intention from their written text is similar to the goal of detecting a communicator's psychological state by counting their usage of pronouns [@Tausczik2009]. Individuals tend to assume they can tell someone else's communicative intention (or someone else's psychological state), and therefore no validation seems necessary. But when this assumption is crosschecked (as in the present study), results indicate that the assumption that we can tell someone else’s communicative intention might very well be biased. Even experts can confidently make incorrect guesses (see the Online Appendix). 

That being said, it is our assumption that the synthetic dataset itself captures the four authors' communicative intentions. Our approach has an important weakness because it creates a "turtles all the way down" situation of who can tell whether the four authors' communicative intentions have been adequately expressed in the content they produced.

As an exploratory analysis, we went down one layer of turtles and allowed the two experts to contest the ground truth. From this ground truth contestation exercise, we learned several lessons about the detection of generic frames and the notion of generic frames in general which could explain our results. 

We found that the original conceptualization of generic frames by @semetko:2000:FE contributes to the difficulty of the whole endeavor of creating articles with a specific frame and finding one frame in a specific article. First, our experts heavily contested articles with the ground truth Morality frame. This comes as no surprise as the detection of the Morality frame also has the worst performance. The extraction of latent moral information from text is a highly subjective task [@weber:2018:ELM] to begin with and this subjectivity contributes not only to the low reliability of traditional content analytic approaches, but also hampers the ability of our coders and experts to determine whether or not any moral message is conveyed (an item in the original codebook). Two experts mainly found that the moral message either does not exist or is implicitly communicated in many of the morality-aimed texts within our corpus. This implicit aspect of morality framing has been expressed in the definition by @semetko:2000:FE, which assumes journalists usually communicate a moral message indirectly [such as raising questions, see @neuman1992common] due to the professional norm of objectivity [^jour]. An example given by @semetko:2000:FE is using the view of an interest group to raise questions about sexually transmitted diseases (rather than what one should do to not get sexually transmitted diseases). In the dataset, there are similar cases where the student authors simply raised questions about morally charged topics, e.g. artificial intelligence, while the moral prescriptions are not clearly communicated. Should we count that as a Morality frame? This question remains unanswered in the literature.

[^jour]: Another weakness of this paper is the fact that the communicators employed for this study are not professional journalists. Journalists might have a better ability to communicate their intentions than our four student authors. Therefore, future research will need to try to replicate this study with journalists. However, as the communicators (student authors) in this study were instructed to write as if they were journalists, the indirectness in moral message expression should also apply to them.

Second, the two experts found that an article can actually contain frame elements of multiple frames. The original operationalization by @semetko:2000:FE actually allows that as their frame measurement is not one score but five scores. In empirical research, however, all the frame elements considered are usually consolidated into just one per news item $\hat{y}$ [e.g. @dirikx:2010:T;@haenens:2001:FD;@kroon:2022:BCW]. The ramification for both manual and automatic coding tasks was that the method to resolve the dominant frame also contributed to accuracy. One can foresee that the performance of any detection method under consideration would be much better if a generic frame is communicated exclusively, without any use of frame elements that belong to one of the other four generic frames. 

We addressed these two issues raised by the two experts with two sensitivity analyses: (1) We evaluated the performance of all methods without all Morality articles, and (2) we evaluated the performance of all methods but using a performance metric that does not assume a dominant frame in each article. These two sensitivity analyses are provided in the Online Appendix. H1 appears to be less supported when all Morality articles are removed. But as with any measurement problem, without further research, it is hard to delineate where the problem originates ---whether the student authors did not communicate the frame clearly, whether coders did not pick them up properly, or, more worryingly, whether there is a conceptual problem with generic frames at the root. We observe that, for example, @burscher:2014:TCC report low intercoder reliability for individual frame elements in real-world news data, and note that while in Framing theory [e.g. @matthes:2014:F, p 15] the assumption is that communicators’ frames feed into the news frames in the texts they write, we rarely validate measured frames against authorial intent. 

This opens up two conceptual questions that can be raised if measurement problems like the ones we observe persist: a) We can ask whether generic frames are indeed too generic. As the expert coders observed, overlap between frames did happen when the student authors incidentally used frame elements of another frame - for example, responsibility attribution could be such a normal part of human communication that it becomes difficult to assess when if happens coincidentally (for example in the lead into a conflict story) or whether it is authorial intent to *highlight* that part of a story. b) We can ask how stringent the assumption of authorial intent in framing is ---or whether a constructivist view of communication which locates the meaning of text in the interpretation of the reader is more appropriate [@luhmann1997gesellschaft, p.72]. That way, the "ground truth" would not be the Infinite Regress of author's intent, but a quality of audience reception. In that case, though, we also have to relax the assumption of framing as a linear process from journalist to audience.

Answering these questions is beyond the scope of this paper. Additional research will have to assess whether methodological or theoretical adjustments are necessary, or whether our corpus included too many outlier articles. We do hope that future research will adapt the construction of synthetic datasets with available ground truth to help find answers or further conceptualize where we expect the ground truth to lie.

## Limitations

One limitation of the paper is that by focusing on generic frames, we have consciously chosen a specifically tough case for automatic (and, it appears: human) methods of frame detection. We suggest further research with issue-specific frames, following a similar setup to ours: Problem definitions would have to be predefined and assigned orthogonally to sub-topics to test if inductive methods can detect authorial intent. 

Another limitation is the sample size of `r nrow(frame_df)`. This limited sample size can have two different implications: (1) whether we have enough variety in articles (e.g. variations in vocabulary, stylistic clues, angles) to use any of the automatic or semi-supervised methods and (2) statistical power of the analysis and/or whether we have enough articles to use any of the automatic or semi-supervised methods. Statistically speaking, increasing the sample size does not always increase variety (if variety means variance, increasing the sample size tends to decrease the variance). In our opinion, a more reliable way to increase the variety of articles is not just to increase the sample size but also to increase the content categories (e.g. topics) or/and authors. With our current data, it is not possible to simulate the possible effect of increasing variety and this warrants further studies.

For the second implication, this study has no say about the equivalence among methods and null. We can only check our superiority hypotheses (H1, H2, and H3).  We refrained from concluding that a method is equivalent to null. The kind of equivalence conclusions can only be drawn with a different study design [see a primer by @weber:2012:TEC]. Suppose one would need to test the equivalence hypothesis, the required sample size would be 13,708 (null value of 20\%; equivalence limit of 2\%; $\alpha$: 0.05; $\beta$: 0.2). To give a perspective to this sample size, the New York Times publishes around 230 new articles per day. The cost to produce this number of articles using our synthetic approach would be equivalent to asking journalists to write two-month worth of news content.

Another issue with the sample size is that automatic and semi-supervised methods studied in this benchmark might not work well with a sample size of `r nrow(frame_df)`, as these methods were not designed to work with this relatively small sample size. In the Online Appendix, an analysis is presented to simulate the possible impact of increasing the sample size on the three hypotheses. Our simulation points to the direction that both H1 and H2 are more likely to be supported when the sample size increases. H3, however, might be less likely to be supported. Therefore, automatic methods are more likely to detect *topics* rather than generic frames when the sample size increases. But we maintain that this finding needs to be confirmed with actual data in a new empirical study.

Finally, an important drawback of our approach of calculating $CCR_{max}$ is that the search space of possible permutations grows factorially, i.e. $k!$. Therefore, the search space is unbearably large when $k$ is just slightly larger than 5 (e.g. $10! = `r factorial(10)`$). It also means that we cannot increase $k$ from 5 to allow for the so-called boilerplate topics  [@maier:2018:ALT] or providing flexibility in frame mapping [@nicholls:2020:CIM;@walter:2019:NFA]. A smarter heuristic algorithm for the calculation of $CCR_{max}$ is needed.

# Concluding remarks

By synthesizing a dataset for the validation of the measurement of generic frames and benchmarking manual and automatic methods with the dataset, we came across problems measuring these frames that, we hope, ignite further discussion on the methodological and theoretical assumptions underlying framing research. Even though the present endeavor might have raised awareness for more issues than it could help resolve, it still provides a number of insights both for framing research and for validity testing in content analysis more broadly. 

Using the example of generic frames, we have demonstrated that in a scenario where different media content features are systematically correlated (in our case, frames and topics) it might be better to work with synthetic corpora that randomize and, thereby, decorrelate, these features to assess the actual validity of detection methods. Our study has also shown that this solution, despite ruling out a number of problems that working with a sample of real-life data has, still comes along with specific issues that are inherent to human content production (but might, likewise, also occur with texts generated by large language models). In our case, this refers to the occurrence of several generic frame elements within one text as well as the journalistic tendency to express morality framing in a way which might be too subtle to be reliably detected in human or machine coding. These shortcomings notwithstanding we believe that the general approach of constructing randomized, synthetic news corpora should be followed upon in future validation studies. Researchers in the field of frame detection should be aware of the limitations addressed here when working with or building upon the corpus published alongside this article.

For framing research in particular, the present findings pose a number of important questions that future research will have to address: Is it fair to assume that individual news items express one dominant or even exclusionary (generic) frame? Is the concept of generic frames actually too generic to be validly detected in content analysis? How can a threshold level for explicitness be defined that allows to assign a specific frame label to a text (considering that Morality frame seems to be expressed mainly implicitly)? And, finally: What are the specific strengths and weaknesses of studying a deductive concept such as frames by using inductive methods such as fully automated clustering?

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
